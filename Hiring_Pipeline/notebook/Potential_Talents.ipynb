{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b7683f",
   "metadata": {},
   "source": [
    "Importing NLP model packages and general packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bf5c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GEOFF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GEOFF\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import isclose\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a0228",
   "metadata": {},
   "source": [
    "Setup your directory to that of the folder containing the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605c72f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = ''\n",
    "os.chdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6d388",
   "metadata": {},
   "source": [
    "Importing the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002e5428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data imported\n",
      "Column 'id' and duplicate rows were removed\n",
      "\n",
      "The dataframe columns and their types are:\n",
      " {'job_title': <class 'str'>, 'location': <class 'str'>, 'connection': <class 'str'>, 'fit': <class 'str'>}\n",
      "\n",
      "The dataframe shape is (53, 4)\n"
     ]
    }
   ],
   "source": [
    "## Import the raw data into a dataframe\n",
    "data_path = \"data/raw/\"\n",
    "dataframe = pd.read_csv(data_path + \"potential-talents - Aspiring human resources - seeking human resources.csv\")\n",
    "print('\\nData imported')\n",
    "dataframe.drop_duplicates(inplace = True, subset = ['job_title', 'location', 'connection'])\n",
    "dataframe.reset_index(drop = True, inplace = True)\n",
    "## Remove a non informative feature\n",
    "dataframe.drop(inplace = True, labels=['id'], axis = 1)\n",
    "print('''Column 'id' and duplicate rows were removed''')\n",
    "##\n",
    "\n",
    "## Display basic information about the dataframe\n",
    "types = [type(c) for c in dataframe.columns]\n",
    "print('\\nThe dataframe columns and their types are:\\n', dict(zip(dataframe.columns, types)))\n",
    "print(f\"\\nThe dataframe shape is {dataframe.shape}\")\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592448a",
   "metadata": {},
   "source": [
    "Pre-processing of the job_title and location strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7ef021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first job title:\n",
      " 2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional\n",
      "The first job title is pre-processed as:\n",
      " bauer colleg busi graduat magna cum laud aspir human resourc profession\n"
     ]
    }
   ],
   "source": [
    "# All the job titles, locations and number of connections will be pre-processed before encoding\n",
    "jobs = list(dataframe['job_title'])\n",
    "locations = list(dataframe['location'])\n",
    "connections = list(dataframe['connection'])\n",
    "processed_jobs = []\n",
    "processed_locations = []\n",
    "processed_connections = np.zeros(len(connections))\n",
    "\n",
    "def process_string(txt):\n",
    "    '''\n",
    "    Process the words in an input string:\n",
    "        - Tokenizes the words in the string\n",
    "        - Removes stopwords, numbers and punctuation\n",
    "        - Performs stemming\n",
    "    \n",
    "    Arguments:\n",
    "        txt: String, for example \"human resources\"\n",
    "    \n",
    "    Returns:\n",
    "        processed: String, for example \"human resourc\"\n",
    "    '''\n",
    "    words = [] # Will contain the words in the txt string\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # Next, tokenize the words\n",
    "    tokens = word_tokenize(txt)\n",
    "    tokens = [i.strip().lower() for i in tokens]\n",
    "    #\n",
    "\n",
    "    # Remove stop words from the list and stems\n",
    "    for word in tokens:\n",
    "        if word == 'hr':\n",
    "            words.append('human'); words.append('resourc')\n",
    "        elif (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation and\n",
    "                    len(word) > 1 and word.isalpha()):  # remove punctuation\n",
    "                stem_word = stemmer.stem(word)  # stemming word\n",
    "                words.append(stem_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "## Generate the processed inputs\n",
    "for job in jobs:\n",
    "    processed_jobs.append(process_string(job))\n",
    "for loc in locations:\n",
    "    processed_locations.append(process_string(loc))\n",
    "for i, con in enumerate(connections):\n",
    "    if con.strip() == '500+':\n",
    "        processed_connections[i] = 1\n",
    "    elif con.strip() == '0':\n",
    "        processed_connections[i] = 0\n",
    "    else:\n",
    "        processed_connections[i] = np.log(int(con))/np.log(500)\n",
    "\n",
    "processed = dict( [ (i, (processed_jobs[i], processed_locations[i], processed_connections[i], 0) ) for i in range(len(connections)) ] )\n",
    "processed_data = pd.DataFrame.from_dict(processed, orient = 'index', columns = ['job_title', 'location', 'log_connections', 'fit'])\n",
    "processed_example = processed_data['job_title'][0]\n",
    "print('The first job title:\\n', dataframe['job_title'][0])\n",
    "print('The first job title is pre-processed as:\\n', processed_example)\n",
    "## Now all the data has been pre-processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813981a8",
   "metadata": {},
   "source": [
    "Encoding of the job_title and location string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820cf5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first job title is encoded as:\n",
      " [-7.44135454e-02  1.78237371e-02 -5.75934462e-02 -4.78980690e-03\n",
      " -1.00911833e-01  4.42423113e-03  2.74604410e-02  6.44031242e-02\n",
      " -1.57705564e-02  1.56021798e-02  3.69933695e-02 -2.18076371e-02\n",
      " -2.17828713e-02 -1.37398625e-02 -6.94503114e-02 -8.66924673e-02\n",
      " -5.50912283e-02  2.65113302e-02 -2.25930791e-02 -3.58368754e-02\n",
      "  6.25475720e-02  1.88039858e-02 -4.58394401e-02 -6.04784004e-02\n",
      " -8.29530433e-02 -3.75333242e-02  2.68533397e-02 -1.24307737e-01\n",
      "  6.33475631e-02 -3.79948653e-02  4.56899107e-02  4.86548506e-02\n",
      "  5.60845509e-02  2.64487378e-02 -9.00991168e-03  5.37304878e-02\n",
      "  5.03494106e-02  5.42962737e-03  1.09710850e-01  4.75321040e-02\n",
      " -6.70789108e-02 -1.35334119e-01 -6.91300184e-02 -4.61152419e-02\n",
      "  3.28364894e-02  3.63205164e-03  2.32290570e-02 -4.58136685e-02\n",
      "  1.44065637e-02  1.70141067e-02 -1.31228015e-01 -3.80313173e-02\n",
      "  6.37280345e-02 -2.71364134e-02 -3.79742086e-02 -4.32213135e-02\n",
      "  2.00519527e-04 -2.86149960e-02 -5.03062131e-03 -5.06998077e-02\n",
      " -2.23302934e-02  2.42061932e-02 -2.22494360e-02  1.27710020e-02\n",
      "  3.39696631e-02 -1.81474146e-02 -4.10230756e-02 -3.99376601e-02\n",
      "  1.34467892e-02 -3.09313629e-02 -1.20348588e-03 -9.91052464e-02\n",
      " -3.00419377e-03  6.23768046e-02  4.23052302e-03 -1.83937345e-02\n",
      " -6.02474026e-02 -2.83140372e-02  1.02035947e-01 -3.74247544e-02\n",
      "  3.64168622e-02  4.69485670e-03 -4.10362855e-02  9.11737457e-02\n",
      " -1.27743371e-02 -2.20878553e-02 -2.32721046e-02 -6.96706073e-03\n",
      " -6.13383157e-03  3.82503420e-02  1.52566331e-02 -8.30481276e-02\n",
      " -1.67095214e-02 -1.54584460e-02 -3.66680212e-02  2.03487091e-02\n",
      " -3.70413251e-02  2.87424214e-02  5.56450859e-02  1.08118849e-02\n",
      " -6.76001459e-02 -1.21753300e-02  2.20756456e-02  4.54923287e-02\n",
      " -7.36214966e-02 -1.78924855e-02 -1.06869079e-02 -3.65750603e-02\n",
      "  4.21710052e-02  3.31151634e-02 -4.74451929e-02 -4.76714373e-02\n",
      " -1.33515716e-01 -3.32866013e-02  8.11852962e-02  3.52867208e-02\n",
      " -7.05574825e-03  2.30535828e-02 -2.22095922e-02 -5.27714901e-02\n",
      "  1.95226520e-02  5.29895611e-02 -5.64456433e-02 -4.97898124e-02\n",
      "  4.05720249e-02 -1.22525327e-01  3.61577421e-02  4.27588642e-33\n",
      " -8.86617005e-02 -2.59284414e-02  1.28268339e-02  5.91556057e-02\n",
      "  1.42933708e-02 -3.52043174e-02 -4.69427183e-02  3.59814875e-02\n",
      "  1.02719456e-01 -2.94464752e-02  3.45591120e-02  5.23329228e-02\n",
      " -4.76126224e-02 -9.26003617e-04 -6.31183311e-02  5.15139289e-02\n",
      " -2.05668155e-02  2.97720619e-02 -4.48098704e-02  6.77596778e-02\n",
      " -1.67536549e-02 -1.79791134e-02  1.91505942e-02 -2.07314417e-02\n",
      " -1.66351628e-02 -1.48182793e-03  4.84687202e-02 -3.38141955e-02\n",
      "  1.03062622e-01  2.93097366e-02  1.04687415e-01  6.70255581e-03\n",
      " -6.81027919e-02 -4.18183915e-02  1.05909696e-02 -1.82978958e-02\n",
      " -2.80150529e-02 -2.66590305e-02 -2.34182342e-03 -3.04826144e-02\n",
      " -3.21863517e-02  4.98042442e-02  9.68093798e-02 -4.94291484e-02\n",
      "  4.78712134e-02  9.94582623e-02  8.12103301e-02  2.94040628e-02\n",
      "  3.79216112e-02  4.35198843e-02 -3.11787669e-02  2.04851422e-02\n",
      " -1.00753224e-02 -1.34445447e-02  1.12901300e-01 -1.64070074e-02\n",
      " -1.17640500e-03  8.05714503e-02 -2.22869646e-02  7.97458645e-03\n",
      "  1.88624822e-02  5.20686284e-02  8.84650461e-03  5.24373576e-02\n",
      "  4.41992469e-02 -6.93495348e-02 -1.56074911e-02 -6.59878319e-03\n",
      "  8.93190876e-02 -1.16470689e-02 -5.58862723e-02  6.13192655e-02\n",
      " -6.49457157e-04  3.61175127e-02 -1.29427284e-01  7.17909113e-02\n",
      "  2.38207169e-02 -9.41130100e-04 -3.35196517e-02  4.63853311e-03\n",
      " -7.63213187e-02  6.59564584e-02  8.20191391e-03 -8.40327591e-02\n",
      "  1.03596166e-01 -1.66758541e-02  3.35029922e-02 -2.46607587e-02\n",
      "  8.85183513e-02  3.95016074e-02 -9.03829280e-03 -1.16130328e-02\n",
      " -6.83976486e-02  8.08698758e-02  8.37247260e-03 -6.15846927e-33\n",
      "  3.71112786e-02 -3.46753336e-02 -7.60801956e-02  6.02140687e-02\n",
      "  5.65570630e-02  5.30358143e-02  6.95422664e-02  1.68753881e-02\n",
      " -1.59287993e-02  1.35090137e-02 -4.05973848e-03 -6.62243664e-02\n",
      "  3.07115670e-02  5.55358976e-02  3.17859501e-02  3.19110788e-02\n",
      "  1.24076484e-02 -3.26249711e-02 -8.39477181e-02  4.05441783e-02\n",
      " -1.52092408e-02  1.08712055e-01 -3.24426815e-02  2.53137331e-02\n",
      " -4.85717133e-02  2.58227736e-02 -5.93341701e-02  4.11484689e-02\n",
      " -9.40274894e-02 -8.04057159e-03  4.95726131e-02 -8.59063957e-03\n",
      " -6.71341196e-02  1.24498541e-02 -5.97912706e-02 -8.59105587e-02\n",
      "  8.39144811e-02  3.71814705e-02 -9.34435800e-03  1.93529911e-02\n",
      "  5.13266549e-02 -3.43584456e-02 -2.01466382e-02 -3.55318859e-02\n",
      "  1.71379242e-02 -1.27600372e-01 -2.61792708e-02 -9.74533334e-02\n",
      "  6.83667231e-03 -8.01775903e-02  5.88894449e-02 -1.60428172e-03\n",
      " -3.33931558e-02 -9.85352546e-02  3.18309851e-02  2.40130275e-02\n",
      "  6.68708310e-02 -1.08156905e-01 -1.97094288e-02  5.88937057e-03\n",
      "  9.43689123e-02  4.46948595e-02  9.65390913e-03  8.30819458e-02\n",
      "  4.99873795e-03 -1.72297359e-02  7.93368369e-03 -9.58393794e-03\n",
      " -3.66720669e-02  5.89536428e-02  8.19615126e-02 -1.58466436e-02\n",
      "  5.42320535e-02 -3.84609029e-03 -1.13479318e-02  1.15433699e-02\n",
      " -4.52011504e-04 -3.17208804e-02 -8.84229224e-03  3.51945311e-03\n",
      " -1.00253537e-01 -1.09234288e-01 -5.10053337e-02  3.46201435e-02\n",
      " -6.35705069e-02  1.64158065e-02  5.81945181e-02 -2.38984264e-02\n",
      " -4.67536069e-04 -6.59095794e-02 -7.55886808e-02  2.19870219e-03\n",
      " -2.64571141e-02  1.27548608e-03 -2.02345680e-02 -2.51893439e-08\n",
      " -7.51669109e-02 -2.21780650e-02  7.06396671e-03 -6.54830888e-04\n",
      " -5.21990657e-03 -8.16054270e-02 -9.36597809e-02 -1.46046244e-02\n",
      " -2.69823726e-02  6.28458261e-02 -6.51572570e-02 -4.21114303e-02\n",
      "  6.13638153e-03  9.06022340e-02  1.14597484e-01  2.38452498e-02\n",
      "  3.39895710e-02  4.58501689e-02 -3.00490819e-02 -1.16254292e-01\n",
      "  1.23426184e-01 -3.70331332e-02 -5.13787083e-02  2.92059910e-02\n",
      " -4.82420065e-03  1.51369823e-02 -1.91865060e-02 -6.87784702e-03\n",
      " -5.11085689e-02  2.24358831e-02  3.60300243e-02  8.63752514e-02\n",
      "  5.64628914e-02 -8.52547660e-02  5.52283116e-02 -1.99114941e-02\n",
      "  7.07093775e-02 -4.49892618e-02 -2.68710405e-02  5.04710712e-02\n",
      "  2.82791965e-02  5.19849174e-02  2.12434884e-02  4.18088734e-02\n",
      "  9.93803740e-02  1.75448731e-02  9.61134955e-03  3.01778745e-02\n",
      " -1.67361610e-02  1.33692753e-02 -7.81453401e-03 -3.06241419e-02\n",
      "  4.41716351e-02 -7.69808237e-03 -3.16107348e-02  5.56293167e-02\n",
      "  1.74870137e-02 -3.73419225e-02 -1.00588866e-01  5.21077514e-02\n",
      "  5.85760437e-02 -3.86915058e-02 -1.05006704e-02 -2.34887302e-02]\n"
     ]
    }
   ],
   "source": [
    "## Defining the embedding model\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_name = 'all-MiniLM-L6-v2' # smaller model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def job_embedding(job):\n",
    "    '''\n",
    "    Returns a previously saved value of the job embedding, or encodes it\n",
    "    \n",
    "    Arguments:\n",
    "        job: String, processed job information\n",
    "    \n",
    "    Returns:\n",
    "        encoding: Array, the encoding of the job\n",
    "    '''\n",
    "    ind = dict(enumerate(np.where(processed_data['job_title'] == job)[0])).get(0)\n",
    "    if encoded and ind != None:\n",
    "        return encoded_data['job_title'].loc[ind]\n",
    "    ind = dict(enumerate(np.where(processed_data['location'] == job)[0])).get(0)\n",
    "    if encoded and ind != None:\n",
    "        return encoded_data['location'].loc[ind]\n",
    "    encoding = model.encode(job, normalize_embeddings = True)\n",
    "    return encoding\n",
    "\n",
    "def encode_jobs():\n",
    "    '''\n",
    "    Encodes all preprocessed jobs and locations.\n",
    "    \n",
    "    Returns:\n",
    "        encoded_data: Pandas dataframe, candidates encoded data\n",
    "    '''\n",
    "    encoded_jobs = np.array([job_embedding(job) for job in processed_jobs])\n",
    "    encoded_locations = np.array([job_embedding(loc) for loc in processed_locations])\n",
    "    data = np.concatenate((encoded_jobs, encoded_locations))\n",
    "    enc_samples = dict( [ (i, (data[i], data[i+len(processed_connections)], processed_connections[i], 0)) for i in range(len(connections)) ] )\n",
    "    encoded_data = pd.DataFrame.from_dict(enc_samples, orient = 'index', columns = ['job_title', 'location', 'log_connections', 'fit'])\n",
    "    return encoded_data\n",
    "\n",
    "## Performs all the encoding\n",
    "encoded = False\n",
    "encoded_data = encode_jobs()\n",
    "encoded = True # Now all the data has been encoded\n",
    "encoded_example = encoded_data['job_title'][0]\n",
    "print('The first job title is encoded as:\\n', encoded_example)\n",
    "job_titles = dataframe['job_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df0919",
   "metadata": {},
   "source": [
    "Defining the way similar candidates are ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb935cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    '''\n",
    "    Returns the cosine similarity between unit vectors\n",
    "    \n",
    "    Arguments:\n",
    "        vec1: Unit array\n",
    "        vec2: Unit array\n",
    "    \n",
    "    Returns:\n",
    "        The numpy dot product\n",
    "    '''\n",
    "    norm1 = np.linalg.norm(vec1); norm2 = np.linalg.norm(vec2)\n",
    "    assert np.isclose(norm1,1); assert np.isclose(norm2,1)\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "def similar_jobs(job, location = None, connec = None, update = False):\n",
    "    '''\n",
    "    Returns a ranking of the candidates best matching a specific job\n",
    "    \n",
    "    Arguments: \n",
    "        job: String or embedding representing the job to match candidates to.\n",
    "             If a string is provided, it will be encoded.\n",
    "             \n",
    "        location: String, a specified city or country\n",
    "        \n",
    "        connec: Integer, number of connections on Linkedin (Max 500)\n",
    "        \n",
    "        update: Boolean, if True dataframes will be sorted according to ranking\n",
    "    \n",
    "    Returns:\n",
    "        ranking: List, a ranking of the 10 best matching candidates\n",
    "    '''\n",
    "    cosine_list = [] # Similarity ranking\n",
    "    if type(job) == str:\n",
    "        emb = job_embedding(job)\n",
    "    else:\n",
    "        emb = job # already encoded\n",
    "    if location != None:\n",
    "        emb += job_embedding(location) # account for location\n",
    "    if connec != None:\n",
    "        emb = np.concatenate((emb, [connec])) # account for connections\n",
    "    emb = np.array(normalize([emb])[0])\n",
    "    # Embedding of input job complete.\n",
    "    ## Next, go through all possible candidates\n",
    "    for i, tup in enumerate(zip(processed_jobs, processed_locations, processed_connections)):\n",
    "        job_, loc_, con_= tup\n",
    "        vec = job_embedding(job_) + (location!=None)*job_embedding(loc_)\n",
    "        if connec != None:\n",
    "            vec = np.concatenate((vec, [con_]))\n",
    "        vec = np.array(normalize([vec])[0])\n",
    "        cosine_list.append(cosine_similarity(emb, vec))\n",
    "    if update:\n",
    "        update_dataframe(cosine_list)\n",
    "    ranking = [[dataframe.index[i]] + dataframe.values[:10].tolist()[i] for i in range(10)]\n",
    "    return ranking\n",
    "\n",
    "def update_dataframe(cosine_list):\n",
    "    '''\n",
    "    Updates candidates dataframe based on provided ranking information.\n",
    "    \n",
    "    Arguments:\n",
    "        cosine_list: List, contains the job cosine similarity of each candidate\n",
    "    \n",
    "    Updates:\n",
    "        The dataframes fit values are updated and sorted in descending order.\n",
    "    '''\n",
    "    for i, cos in enumerate(cosine_list):\n",
    "        dataframe.loc[i, ['fit']] = cos\n",
    "        processed_data.loc[i, ['fit']] = cos\n",
    "        encoded_data.loc[i, ['fit']] = cos\n",
    "    dataframe.sort_values(by=['fit'], ascending = False, inplace = True)\n",
    "    processed_data.sort_values(by=['fit'], ascending = False, inplace = True)\n",
    "    encoded_data.sort_values(by=['fit'], ascending = False, inplace = True)\n",
    "    return None\n",
    "\n",
    "def star_rank(keyword, star = [], location = None, connec = None, weights = [0.4, 0.5, 0.1]):\n",
    "    '''\n",
    "    Provides a candidate ranking based on job characteristics and manual \n",
    "    supervisory signal (history of starred candidates).\n",
    "    \n",
    "    Arguments:\n",
    "        keyword: String, describes the job to match candidates to.\n",
    "        star: List, indices of candidates starred in chronological order\n",
    "        location: String, preferred location\n",
    "        connec: Integer, number of Linkedin connections (Max 500)\n",
    "        weights: List [a, b, c], importance accorded to (a+b+c=1): \n",
    "                    a: keyword provided\n",
    "                        and manual supervisory signals:\n",
    "                    b: last starred candidate\n",
    "                    c: previously starred candidates\n",
    "    \n",
    "    Returns:\n",
    "        ranking: List, ranking of candidates\n",
    "    '''\n",
    "    key_emb = job_embedding(process_string(keyword))\n",
    "    key_w, star_w, prestar_w = weights # Importance accredited to keyword vs starred vs previously starred\n",
    "    if len(star) > 0:\n",
    "        star_emb = job_embedding(star.pop())\n",
    "        if len(star) > 0:\n",
    "            prestar_emb = np.average([job_embedding(s) for s in star], axis = 0)\n",
    "            prestar_emb = np.squeeze(normalize(prestar_emb.reshape(1,-1), axis = 1))\n",
    "            vec = key_w*key_emb + star_w*star_emb + prestar_w*prestar_emb\n",
    "        else:\n",
    "            vec = key_w*key_emb + star_w*star_emb\n",
    "    else:\n",
    "        vec = key_emb\n",
    "    # Now the vector vec is an amalgation of the embeddings of the keyword + starred candidates\n",
    "    vec = np.squeeze(normalize(vec.reshape(1,-1), axis = 1))\n",
    "    ranking = similar_jobs(vec, location = location, connec = connec, update = True)\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88e724",
   "metadata": {},
   "source": [
    "Below are the executive functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a7665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_ind = [] # List of indices of starred candidates\n",
    "\n",
    "def reset_starring():\n",
    "    'Resets / Erases all manual supervisory signals previously provided.'\n",
    "    star_ind = []\n",
    "    return None\n",
    "\n",
    "def star(ind):\n",
    "    '''\n",
    "    Stars a candidate\n",
    "    \n",
    "    Arguments:\n",
    "        ind: Integer, candidate index in the dataframe\n",
    "    \n",
    "    Performs:\n",
    "        Candidate starring.\n",
    "    '''\n",
    "    star_ind.append(ind)\n",
    "    job, loc = dataframe['job_title'][ind], dataframe['location'][ind]\n",
    "    print('The candidate %s was starred with index %s'%((job, loc), ind))\n",
    "    return None\n",
    "\n",
    "def rank(keyword, location = None):\n",
    "    '''\n",
    "    Simplified use of candidate ranking operation.\n",
    "    \n",
    "    Arguments:\n",
    "        keyword: String, describes the job to match candidates to.\n",
    "        location: String, preferred location\n",
    "                  If not provided, the location of the last candidate \n",
    "                  to be starred will be used.\n",
    "    \n",
    "    Returns:\n",
    "        ranking: List, ranking of candidates\n",
    "    '''\n",
    "    print('\\nThe keyword provided is: '+keyword)\n",
    "    processed_starred = [processed_jobs[ind] for ind in star_ind]\n",
    "    location, connec = None, None\n",
    "    if len(star_ind) > 0:\n",
    "        if location == None:\n",
    "            location = processed_locations[star_ind[-1]]\n",
    "        connec = processed_connections[star_ind[-1]]\n",
    "    ranking = star_rank(keyword, star = processed_starred.copy(), location = location, connec = connec)\n",
    "    print('\\nRanking with fit probability:\\n')\n",
    "    for ranked in ranking:\n",
    "        print(*zip(['index']+list(dataframe.columns), ranked))\n",
    "    print('\\nConsult the dataframe variable for the complete list\\n')\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9cd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_starring() ## This command clears any previous starring action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb52615",
   "metadata": {},
   "source": [
    "Below, the keyword can be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81b28d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now providing the keywords\n",
    "keywords = ['Aspiring human resources',  'seeking human resources']\n",
    "keyword = keywords[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7540bfd9",
   "metadata": {},
   "source": [
    "Below, a ranking based purely on the above-specified keyword is made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b42377b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The keyword provided is: seeking human resources\n",
      "\n",
      "Ranking with fit probability:\n",
      "\n",
      "('index', 13) ('job_title', 'Seeking Human Resources Opportunities') ('location', 'Chicago, Illinois') ('connection', '390') ('fit', 0.9503727114909426)\n",
      "('index', 47) ('job_title', 'Seeking Human Resources Position') ('location', 'Las Vegas, Nevada Area') ('connection', '48') ('fit', 0.9475734174476902)\n",
      "('index', 48) ('job_title', 'Aspiring Human Resources Manager | Graduating May 2020 | Seeking an Entry-Level Human Resources Position in St. Louis') ('location', 'Cape Girardeau, Missouri') ('connection', '103') ('fit', 0.7977208679184986)\n",
      "('index', 8) ('job_title', 'Seeking Human Resources HRIS and Generalist Positions') ('location', 'Greater Philadelphia Area') ('connection', '500+ ') ('fit', 0.7272268794421783)\n",
      "('index', 22) ('job_title', 'Human Resources Professional') ('location', 'Greater Boston Area') ('connection', '16') ('fit', 0.6782880212393358)\n",
      "('index', 42) ('job_title', 'Seeking Human  Resources Opportunities. Open to travel and relocation.') ('location', 'Amerika Birleşik Devletleri') ('connection', '415') ('fit', 0.6700672426753638)\n",
      "('index', 2) ('job_title', 'Aspiring Human Resources Professional') ('location', 'Raleigh-Durham, North Carolina Area') ('connection', '44') ('fit', 0.619956691518524)\n",
      "('index', 45) ('job_title', 'Aspiring Human Resources Professional') ('location', 'Kokomo, Indiana Area') ('connection', '71') ('fit', 0.619956691518524)\n",
      "('index', 21) ('job_title', 'Aspiring Human Resources Manager, seeking internship in Human Resources.') ('location', 'Houston, Texas Area') ('connection', '7') ('fit', 0.6001988774713389)\n",
      "('index', 5) ('job_title', 'Aspiring Human Resources Specialist') ('location', 'Greater New York City Area') ('connection', '1') ('fit', 0.5991128881995613)\n",
      "\n",
      "Consult the dataframe variable for the complete list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ranking_nostar = rank(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f818af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The candidate (\"Human Resources Generalist at Schwan's\", 'Amerika Birleşik Devletleri') was starred with index 26\n"
     ]
    }
   ],
   "source": [
    "# Star a candidate\n",
    "star(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578d107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The keyword provided is: seeking human resources\n",
      "\n",
      "Ranking with fit probability:\n",
      "\n",
      "('index', 26) ('job_title', \"Human Resources Generalist at Schwan's\") ('location', 'Amerika Birleşik Devletleri') ('connection', '500+ ') ('fit', 0.966179414774744)\n",
      "('index', 1) ('job_title', 'Native English Teacher at EPIK (English Program in Korea)') ('location', 'Kanada') ('connection', '500+ ') ('fit', 0.931895269068265)\n",
      "('index', 42) ('job_title', 'Seeking Human  Resources Opportunities. Open to travel and relocation.') ('location', 'Amerika Birleşik Devletleri') ('connection', '415') ('fit', 0.8527217011678001)\n",
      "('index', 6) ('job_title', 'Student at Humber College and Aspiring Human Resources Generalist') ('location', 'Kanada') ('connection', '61') ('fit', 0.8187394902202373)\n",
      "('index', 30) ('job_title', 'Aspiring Human Resources Professional | An energetic and Team-Focused Leader') ('location', 'Austin, Texas Area') ('connection', '174') ('fit', 0.7709154404922836)\n",
      "('index', 3) ('job_title', 'People Development Coordinator at Ryan') ('location', 'Denton, Texas') ('connection', '500+ ') ('fit', 0.7417247267398859)\n",
      "('index', 12) ('job_title', 'Aspiring Human Resources Management student seeking an internship') ('location', 'Houston, Texas Area') ('connection', '500+ ') ('fit', 0.7261656573420008)\n",
      "('index', 23) ('job_title', 'Nortia Staffing is seeking Human Resources, Payroll & Administrative Professionals!!  (408) 709-2621') ('location', 'San Jose, California') ('connection', '500+ ') ('fit', 0.7102620054230072)\n",
      "('index', 7) ('job_title', 'HR Senior Specialist') ('location', 'San Francisco Bay Area') ('connection', '500+ ') ('fit', 0.7058874747563155)\n",
      "('index', 51) ('job_title', 'Always set them up for Success') ('location', 'Greater Los Angeles Area') ('connection', '500+ ') ('fit', 0.7030506668671891)\n",
      "\n",
      "Consult the dataframe variable for the complete list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rank the candidates based on the keyword + starred candidate\n",
    "ranking = rank(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e26e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The candidate ('Aspiring Human Resources Manager, seeking internship in Human Resources.', 'Houston, Texas Area') was starred with index 21\n"
     ]
    }
   ],
   "source": [
    "# Star another candidate\n",
    "star(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23fce033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The keyword provided is: seeking human resources\n",
      "\n",
      "Ranking with fit probability:\n",
      "\n",
      "('index', 21) ('job_title', 'Aspiring Human Resources Manager, seeking internship in Human Resources.') ('location', 'Houston, Texas Area') ('connection', '7') ('fit', 0.9562294330572801)\n",
      "('index', 12) ('job_title', 'Aspiring Human Resources Management student seeking an internship') ('location', 'Houston, Texas Area') ('connection', '500+ ') ('fit', 0.8378351165914746)\n",
      "('index', 10) ('job_title', 'SVP, CHRO, Marketing & Communications, CSR Officer | ENGIE | Houston | The Woodlands | Energy | GPHR | SPHR') ('location', 'Houston, Texas Area') ('connection', '500+ ') ('fit', 0.7562853616795456)\n",
      "('index', 46) ('job_title', 'Student') ('location', 'Houston, Texas Area') ('connection', '4') ('fit', 0.7276595637416854)\n",
      "('index', 1) ('job_title', 'Native English Teacher at EPIK (English Program in Korea)') ('location', 'Kanada') ('connection', '500+ ') ('fit', 0.6812084567828091)\n",
      "('index', 6) ('job_title', 'Student at Humber College and Aspiring Human Resources Generalist') ('location', 'Kanada') ('connection', '61') ('fit', 0.6786394561371221)\n",
      "('index', 29) ('job_title', 'Senior Human Resources Business Partner at Heil Environmental') ('location', 'Chattanooga, Tennessee Area') ('connection', '455') ('fit', 0.6419002875047164)\n",
      "('index', 36) ('job_title', 'Human Resources Management Major') ('location', 'Milpitas, California') ('connection', '18') ('fit', 0.6243518426622723)\n",
      "('index', 42) ('job_title', 'Seeking Human  Resources Opportunities. Open to travel and relocation.') ('location', 'Amerika Birleşik Devletleri') ('connection', '415') ('fit', 0.61341885054032)\n",
      "('index', 26) ('job_title', \"Human Resources Generalist at Schwan's\") ('location', 'Amerika Birleşik Devletleri') ('connection', '500+ ') ('fit', 0.612689025403742)\n",
      "\n",
      "Consult the dataframe variable for the complete list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-rank based on the keyword + starred candidate (and learning from previously starred candidate(s))\n",
    "ranking = rank(keyword)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
